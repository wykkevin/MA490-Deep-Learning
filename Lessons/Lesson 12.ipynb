{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D    20\n",
       "J    14\n",
       "G    14\n",
       "I    12\n",
       "E     9\n",
       "A     8\n",
       "C     7\n",
       "H     7\n",
       "B     5\n",
       "F     4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notMNIST = np.load('notMNIST_train_100.npz')\n",
    "\n",
    "images = notMNIST['train_images']\n",
    "labels = notMNIST['train_labels']\n",
    "\n",
    "label_names = np.array(['A','B','C','D','E','F','G','H','I','J'])\n",
    "pd.Series(label_names[labels]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = images.reshape(100,28*28)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = pd.get_dummies(labels).values\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 195.8909 - acc: 0.1000 - val_loss: 181.3040 - val_acc: 0.1000\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 0s 190us/step - loss: 176.4276 - acc: 0.1125 - val_loss: 162.9029 - val_acc: 0.1000\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 0s 213us/step - loss: 158.4055 - acc: 0.1125 - val_loss: 145.9325 - val_acc: 0.1000\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 141.8023 - acc: 0.1125 - val_loss: 130.3605 - val_acc: 0.1000\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 126.5859 - acc: 0.1250 - val_loss: 116.1404 - val_acc: 0.1000\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 112.7044 - acc: 0.1250 - val_loss: 103.2136 - val_acc: 0.1000\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 0s 227us/step - loss: 100.0971 - acc: 0.1375 - val_loss: 91.5113 - val_acc: 0.1000\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 88.6919 - acc: 0.1500 - val_loss: 80.9570 - val_acc: 0.1000\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 0s 211us/step - loss: 78.4150 - acc: 0.1500 - val_loss: 71.4733 - val_acc: 0.1000\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 69.1870 - acc: 0.1500 - val_loss: 62.9788 - val_acc: 0.1500\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 60.9215 - acc: 0.1500 - val_loss: 55.3399 - val_acc: 0.1500\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 0s 190us/step - loss: 53.5035 - acc: 0.1500 - val_loss: 48.5972 - val_acc: 0.1500\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 0s 238us/step - loss: 46.9542 - acc: 0.1500 - val_loss: 42.6150 - val_acc: 0.1500\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 0s 196us/step - loss: 41.1439 - acc: 0.1500 - val_loss: 37.3206 - val_acc: 0.1500\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 36.0083 - acc: 0.1875 - val_loss: 32.6478 - val_acc: 0.1500\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 0s 213us/step - loss: 31.4802 - acc: 0.2250 - val_loss: 28.5391 - val_acc: 0.1500\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 0s 220us/step - loss: 27.4891 - acc: 0.3000 - val_loss: 24.9175 - val_acc: 0.1500\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 23.9851 - acc: 0.3375 - val_loss: 21.7491 - val_acc: 0.1500\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 0s 220us/step - loss: 20.9244 - acc: 0.3625 - val_loss: 18.9838 - val_acc: 0.1500\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 0s 189us/step - loss: 18.2501 - acc: 0.3750 - val_loss: 16.5838 - val_acc: 0.2000\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 15.9292 - acc: 0.4125 - val_loss: 14.4988 - val_acc: 0.2000\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 0s 180us/step - loss: 13.9120 - acc: 0.4625 - val_loss: 12.6919 - val_acc: 0.2000\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 0s 235us/step - loss: 12.1650 - acc: 0.4875 - val_loss: 11.1316 - val_acc: 0.2500\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 10.6558 - acc: 0.5250 - val_loss: 9.7848 - val_acc: 0.2500\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 0s 188us/step - loss: 9.3561 - acc: 0.5375 - val_loss: 8.6268 - val_acc: 0.2500\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 8.2365 - acc: 0.5875 - val_loss: 7.6350 - val_acc: 0.2500\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 0s 174us/step - loss: 7.2778 - acc: 0.6000 - val_loss: 6.7875 - val_acc: 0.2500\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 0s 177us/step - loss: 6.4578 - acc: 0.6000 - val_loss: 6.0640 - val_acc: 0.2500\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 0s 229us/step - loss: 5.7577 - acc: 0.6000 - val_loss: 5.4475 - val_acc: 0.3000\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 0s 211us/step - loss: 5.1635 - acc: 0.5875 - val_loss: 4.9246 - val_acc: 0.3000\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 4.6578 - acc: 0.5500 - val_loss: 4.4781 - val_acc: 0.3000\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 4.2317 - acc: 0.5500 - val_loss: 4.1027 - val_acc: 0.3000\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 3.8712 - acc: 0.5625 - val_loss: 3.7888 - val_acc: 0.2500\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 3.5667 - acc: 0.5750 - val_loss: 3.5316 - val_acc: 0.2500\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 0s 236us/step - loss: 3.3144 - acc: 0.5750 - val_loss: 3.3174 - val_acc: 0.2000\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 3.1015 - acc: 0.5750 - val_loss: 3.1344 - val_acc: 0.1500\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 0s 174us/step - loss: 2.9245 - acc: 0.5625 - val_loss: 2.9806 - val_acc: 0.2000\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.7784 - acc: 0.5750 - val_loss: 2.8533 - val_acc: 0.2000\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 0s 190us/step - loss: 2.6574 - acc: 0.5625 - val_loss: 2.7488 - val_acc: 0.1500\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 0s 221us/step - loss: 2.5572 - acc: 0.5875 - val_loss: 2.6653 - val_acc: 0.2000\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.4741 - acc: 0.5875 - val_loss: 2.5968 - val_acc: 0.1500\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.4064 - acc: 0.5625 - val_loss: 2.5439 - val_acc: 0.1500\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 2.3511 - acc: 0.5375 - val_loss: 2.5000 - val_acc: 0.1500\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.3066 - acc: 0.5375 - val_loss: 2.4607 - val_acc: 0.1500\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 0s 206us/step - loss: 2.2692 - acc: 0.5375 - val_loss: 2.4287 - val_acc: 0.1500\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 0s 200us/step - loss: 2.2402 - acc: 0.5375 - val_loss: 2.4015 - val_acc: 0.1500\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 0s 183us/step - loss: 2.2168 - acc: 0.5500 - val_loss: 2.3805 - val_acc: 0.1500\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.1979 - acc: 0.5375 - val_loss: 2.3678 - val_acc: 0.1500\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.1817 - acc: 0.5250 - val_loss: 2.3578 - val_acc: 0.1500\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 2.1693 - acc: 0.5375 - val_loss: 2.3516 - val_acc: 0.1500\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 2.1590 - acc: 0.5500 - val_loss: 2.3460 - val_acc: 0.1500\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.1516 - acc: 0.5500 - val_loss: 2.3407 - val_acc: 0.1500\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.1442 - acc: 0.5375 - val_loss: 2.3369 - val_acc: 0.1500\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 0s 198us/step - loss: 2.1397 - acc: 0.5375 - val_loss: 2.3334 - val_acc: 0.1500\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 0s 176us/step - loss: 2.1362 - acc: 0.5375 - val_loss: 2.3287 - val_acc: 0.1500\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 0s 196us/step - loss: 2.1319 - acc: 0.5375 - val_loss: 2.3245 - val_acc: 0.1500\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 0s 237us/step - loss: 2.1294 - acc: 0.5375 - val_loss: 2.3223 - val_acc: 0.1500\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.1274 - acc: 0.5375 - val_loss: 2.3237 - val_acc: 0.1500\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 2.1263 - acc: 0.5250 - val_loss: 2.3251 - val_acc: 0.1500\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 0s 174us/step - loss: 2.1249 - acc: 0.5250 - val_loss: 2.3258 - val_acc: 0.2000\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 187us/step - loss: 2.1228 - acc: 0.5500 - val_loss: 2.3239 - val_acc: 0.1500\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 0s 206us/step - loss: 2.1225 - acc: 0.5375 - val_loss: 2.3228 - val_acc: 0.1500\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 0s 237us/step - loss: 2.1207 - acc: 0.5375 - val_loss: 2.3175 - val_acc: 0.1500\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 0s 186us/step - loss: 2.1195 - acc: 0.5500 - val_loss: 2.3145 - val_acc: 0.1500\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 0s 174us/step - loss: 2.1189 - acc: 0.5500 - val_loss: 2.3152 - val_acc: 0.2000\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 0s 174us/step - loss: 2.1199 - acc: 0.5500 - val_loss: 2.3181 - val_acc: 0.2000\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 0s 201us/step - loss: 2.1194 - acc: 0.5500 - val_loss: 2.3178 - val_acc: 0.2000\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 0s 201us/step - loss: 2.1178 - acc: 0.5500 - val_loss: 2.3167 - val_acc: 0.1500\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.1176 - acc: 0.5250 - val_loss: 2.3166 - val_acc: 0.1500\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 0s 237us/step - loss: 2.1167 - acc: 0.5250 - val_loss: 2.3177 - val_acc: 0.1500\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 0s 201us/step - loss: 2.1161 - acc: 0.5125 - val_loss: 2.3220 - val_acc: 0.1500\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 2.1152 - acc: 0.5125 - val_loss: 2.3216 - val_acc: 0.1500\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 0s 191us/step - loss: 2.1146 - acc: 0.5125 - val_loss: 2.3242 - val_acc: 0.1500\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.1148 - acc: 0.5125 - val_loss: 2.3266 - val_acc: 0.1500\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.1139 - acc: 0.5250 - val_loss: 2.3275 - val_acc: 0.1500\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.1132 - acc: 0.5125 - val_loss: 2.3248 - val_acc: 0.1500\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 0s 184us/step - loss: 2.1131 - acc: 0.5250 - val_loss: 2.3227 - val_acc: 0.1500\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 0s 238us/step - loss: 2.1124 - acc: 0.5250 - val_loss: 2.3210 - val_acc: 0.1500\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 2.1119 - acc: 0.5250 - val_loss: 2.3190 - val_acc: 0.1500\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.1114 - acc: 0.5125 - val_loss: 2.3204 - val_acc: 0.1500\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.1110 - acc: 0.5250 - val_loss: 2.3187 - val_acc: 0.1500\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 2.1101 - acc: 0.5250 - val_loss: 2.3169 - val_acc: 0.1500\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 0s 241us/step - loss: 2.1105 - acc: 0.5250 - val_loss: 2.3157 - val_acc: 0.1500\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.1110 - acc: 0.5125 - val_loss: 2.3167 - val_acc: 0.1500\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 0s 218us/step - loss: 2.1103 - acc: 0.5125 - val_loss: 2.3191 - val_acc: 0.1500\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 0s 188us/step - loss: 2.1090 - acc: 0.5125 - val_loss: 2.3238 - val_acc: 0.1500\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 0s 184us/step - loss: 2.1087 - acc: 0.5250 - val_loss: 2.3247 - val_acc: 0.1500\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.1083 - acc: 0.5250 - val_loss: 2.3215 - val_acc: 0.1500\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 2.1078 - acc: 0.5250 - val_loss: 2.3186 - val_acc: 0.1500\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.1076 - acc: 0.5250 - val_loss: 2.3167 - val_acc: 0.1500\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.1070 - acc: 0.5250 - val_loss: 2.3154 - val_acc: 0.1500\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 0s 198us/step - loss: 2.1061 - acc: 0.5250 - val_loss: 2.3195 - val_acc: 0.1500\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 0s 237us/step - loss: 2.1063 - acc: 0.5125 - val_loss: 2.3223 - val_acc: 0.1500\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.1061 - acc: 0.5250 - val_loss: 2.3253 - val_acc: 0.1500\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 215us/step - loss: 2.1049 - acc: 0.5125 - val_loss: 2.3251 - val_acc: 0.1500\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.1055 - acc: 0.5000 - val_loss: 2.3265 - val_acc: 0.1500\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 0s 223us/step - loss: 2.1047 - acc: 0.5125 - val_loss: 2.3249 - val_acc: 0.1500\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 0s 223us/step - loss: 2.1040 - acc: 0.5125 - val_loss: 2.3228 - val_acc: 0.1500\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 0s 198us/step - loss: 2.1024 - acc: 0.5125 - val_loss: 2.3236 - val_acc: 0.1500\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.1026 - acc: 0.5250 - val_loss: 2.3263 - val_acc: 0.1500\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.1030 - acc: 0.5250 - val_loss: 2.3275 - val_acc: 0.1500\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.1027 - acc: 0.5250 - val_loss: 2.3268 - val_acc: 0.1500\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 0s 165us/step - loss: 2.1023 - acc: 0.5125 - val_loss: 2.3259 - val_acc: 0.1500\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.1012 - acc: 0.5125 - val_loss: 2.3210 - val_acc: 0.1500\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 0s 186us/step - loss: 2.0998 - acc: 0.5250 - val_loss: 2.3220 - val_acc: 0.1500\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 0s 198us/step - loss: 2.1011 - acc: 0.5125 - val_loss: 2.3250 - val_acc: 0.1500\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 2.1000 - acc: 0.5125 - val_loss: 2.3270 - val_acc: 0.1500\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0997 - acc: 0.5125 - val_loss: 2.3285 - val_acc: 0.1500\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.1007 - acc: 0.5125 - val_loss: 2.3235 - val_acc: 0.1500\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 0s 214us/step - loss: 2.0995 - acc: 0.5125 - val_loss: 2.3219 - val_acc: 0.1500\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 0s 198us/step - loss: 2.0997 - acc: 0.5125 - val_loss: 2.3255 - val_acc: 0.1500\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 2.0989 - acc: 0.5125 - val_loss: 2.3277 - val_acc: 0.1500\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 0s 198us/step - loss: 2.0980 - acc: 0.5250 - val_loss: 2.3244 - val_acc: 0.1500\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0975 - acc: 0.5250 - val_loss: 2.3197 - val_acc: 0.1500\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0971 - acc: 0.5250 - val_loss: 2.3195 - val_acc: 0.1500\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0977 - acc: 0.5125 - val_loss: 2.3211 - val_acc: 0.1500\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 0s 174us/step - loss: 2.0962 - acc: 0.5125 - val_loss: 2.3228 - val_acc: 0.1500\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 0s 235us/step - loss: 2.0954 - acc: 0.5125 - val_loss: 2.3254 - val_acc: 0.1500\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 0s 226us/step - loss: 2.0956 - acc: 0.5250 - val_loss: 2.3260 - val_acc: 0.1500\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0959 - acc: 0.5125 - val_loss: 2.3263 - val_acc: 0.1500\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 0s 210us/step - loss: 2.0949 - acc: 0.5125 - val_loss: 2.3262 - val_acc: 0.1500\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 226us/step - loss: 2.0949 - acc: 0.5000 - val_loss: 2.3240 - val_acc: 0.1500\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0938 - acc: 0.5125 - val_loss: 2.3208 - val_acc: 0.1500\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0939 - acc: 0.5125 - val_loss: 2.3200 - val_acc: 0.1500\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0933 - acc: 0.5125 - val_loss: 2.3171 - val_acc: 0.1500\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 0s 215us/step - loss: 2.0929 - acc: 0.5125 - val_loss: 2.3189 - val_acc: 0.1500\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0924 - acc: 0.5125 - val_loss: 2.3193 - val_acc: 0.1500\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 0s 219us/step - loss: 2.0923 - acc: 0.5250 - val_loss: 2.3213 - val_acc: 0.1500\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 0s 223us/step - loss: 2.0924 - acc: 0.5250 - val_loss: 2.3233 - val_acc: 0.1500\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 0s 201us/step - loss: 2.0916 - acc: 0.5125 - val_loss: 2.3231 - val_acc: 0.1500\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.0908 - acc: 0.5125 - val_loss: 2.3270 - val_acc: 0.1500\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0914 - acc: 0.5125 - val_loss: 2.3312 - val_acc: 0.1500\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0918 - acc: 0.5250 - val_loss: 2.3278 - val_acc: 0.1500\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 0s 210us/step - loss: 2.0913 - acc: 0.5125 - val_loss: 2.3249 - val_acc: 0.1500\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 0s 248us/step - loss: 2.0910 - acc: 0.5125 - val_loss: 2.3229 - val_acc: 0.1500\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 0s 240us/step - loss: 2.0897 - acc: 0.5125 - val_loss: 2.3235 - val_acc: 0.1500\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.0897 - acc: 0.5250 - val_loss: 2.3268 - val_acc: 0.1500\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 0s 235us/step - loss: 2.0898 - acc: 0.5125 - val_loss: 2.3336 - val_acc: 0.1500\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 0s 215us/step - loss: 2.0887 - acc: 0.5125 - val_loss: 2.3335 - val_acc: 0.1500\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 0s 211us/step - loss: 2.0889 - acc: 0.5125 - val_loss: 2.3292 - val_acc: 0.1500\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 0s 206us/step - loss: 2.0883 - acc: 0.5000 - val_loss: 2.3244 - val_acc: 0.1500\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 2.0890 - acc: 0.5125 - val_loss: 2.3221 - val_acc: 0.1500\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.0884 - acc: 0.5125 - val_loss: 2.3247 - val_acc: 0.1500\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0874 - acc: 0.5125 - val_loss: 2.3258 - val_acc: 0.1500\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 186us/step - loss: 2.0873 - acc: 0.5125 - val_loss: 2.3259 - val_acc: 0.1500\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 201us/step - loss: 2.0868 - acc: 0.5125 - val_loss: 2.3262 - val_acc: 0.1500\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0863 - acc: 0.5125 - val_loss: 2.3259 - val_acc: 0.2000\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 2.0875 - acc: 0.5125 - val_loss: 2.3282 - val_acc: 0.1500\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 0s 200us/step - loss: 2.0865 - acc: 0.5250 - val_loss: 2.3306 - val_acc: 0.1500\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.0865 - acc: 0.5125 - val_loss: 2.3280 - val_acc: 0.1500\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0856 - acc: 0.5125 - val_loss: 2.3283 - val_acc: 0.1500\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 2.0851 - acc: 0.5125 - val_loss: 2.3281 - val_acc: 0.1500\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.0845 - acc: 0.5125 - val_loss: 2.3289 - val_acc: 0.1500\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 0s 229us/step - loss: 2.0842 - acc: 0.5125 - val_loss: 2.3305 - val_acc: 0.1500\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0843 - acc: 0.5125 - val_loss: 2.3275 - val_acc: 0.2000\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 0s 213us/step - loss: 2.0838 - acc: 0.5125 - val_loss: 2.3255 - val_acc: 0.2000\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 0s 206us/step - loss: 2.0842 - acc: 0.5125 - val_loss: 2.3245 - val_acc: 0.2000\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0841 - acc: 0.5125 - val_loss: 2.3285 - val_acc: 0.1500\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.0827 - acc: 0.5125 - val_loss: 2.3313 - val_acc: 0.1500\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 0s 225us/step - loss: 2.0825 - acc: 0.5125 - val_loss: 2.3347 - val_acc: 0.1500\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 0s 236us/step - loss: 2.0830 - acc: 0.5125 - val_loss: 2.3334 - val_acc: 0.1500\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0830 - acc: 0.5125 - val_loss: 2.3330 - val_acc: 0.1500\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0826 - acc: 0.5125 - val_loss: 2.3281 - val_acc: 0.1500\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 0s 200us/step - loss: 2.0819 - acc: 0.5125 - val_loss: 2.3233 - val_acc: 0.2000\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 2.0813 - acc: 0.5125 - val_loss: 2.3229 - val_acc: 0.2000\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 0s 216us/step - loss: 2.0812 - acc: 0.5125 - val_loss: 2.3259 - val_acc: 0.2000\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.0801 - acc: 0.5125 - val_loss: 2.3277 - val_acc: 0.2000\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 0s 174us/step - loss: 2.0808 - acc: 0.5125 - val_loss: 2.3270 - val_acc: 0.2000\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 0s 207us/step - loss: 2.0805 - acc: 0.5125 - val_loss: 2.3251 - val_acc: 0.1500\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 185us/step - loss: 2.0807 - acc: 0.5125 - val_loss: 2.3264 - val_acc: 0.1500\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0800 - acc: 0.5125 - val_loss: 2.3259 - val_acc: 0.1500\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.0802 - acc: 0.5125 - val_loss: 2.3303 - val_acc: 0.1500\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0790 - acc: 0.5125 - val_loss: 2.3313 - val_acc: 0.2000\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 0s 195us/step - loss: 2.0796 - acc: 0.5125 - val_loss: 2.3337 - val_acc: 0.2000\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 0s 225us/step - loss: 2.0798 - acc: 0.5125 - val_loss: 2.3339 - val_acc: 0.1500\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0783 - acc: 0.5125 - val_loss: 2.3266 - val_acc: 0.2000\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 2.0800 - acc: 0.5125 - val_loss: 2.3217 - val_acc: 0.2000\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 0s 249us/step - loss: 2.0798 - acc: 0.5125 - val_loss: 2.3211 - val_acc: 0.2000\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 0s 211us/step - loss: 2.0780 - acc: 0.5125 - val_loss: 2.3229 - val_acc: 0.2000\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 0s 198us/step - loss: 2.0783 - acc: 0.5125 - val_loss: 2.3298 - val_acc: 0.1500\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 0s 200us/step - loss: 2.0791 - acc: 0.5125 - val_loss: 2.3353 - val_acc: 0.1500\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 2.0787 - acc: 0.5000 - val_loss: 2.3315 - val_acc: 0.1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0786 - acc: 0.5125 - val_loss: 2.3274 - val_acc: 0.1500\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 0s 182us/step - loss: 2.0779 - acc: 0.5125 - val_loss: 2.3277 - val_acc: 0.1500\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0765 - acc: 0.5125 - val_loss: 2.3298 - val_acc: 0.1500\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 0s 197us/step - loss: 2.0778 - acc: 0.5125 - val_loss: 2.3300 - val_acc: 0.2000\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 2.0762 - acc: 0.5125 - val_loss: 2.3334 - val_acc: 0.1500\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 0s 174us/step - loss: 2.0762 - acc: 0.5250 - val_loss: 2.3350 - val_acc: 0.1500\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 0s 200us/step - loss: 2.0759 - acc: 0.5125 - val_loss: 2.3373 - val_acc: 0.1500\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0768 - acc: 0.5125 - val_loss: 2.3355 - val_acc: 0.1500\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 0s 214us/step - loss: 2.0775 - acc: 0.5250 - val_loss: 2.3310 - val_acc: 0.1500\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 0s 205us/step - loss: 2.0761 - acc: 0.5125 - val_loss: 2.3332 - val_acc: 0.1500\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 2.0755 - acc: 0.5125 - val_loss: 2.3379 - val_acc: 0.1500\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 0s 213us/step - loss: 2.0762 - acc: 0.5125 - val_loss: 2.3386 - val_acc: 0.1500\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 0s 198us/step - loss: 2.0754 - acc: 0.5125 - val_loss: 2.3396 - val_acc: 0.1500\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 0s 217us/step - loss: 2.0746 - acc: 0.5125 - val_loss: 2.3391 - val_acc: 0.1500\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 0s 219us/step - loss: 2.0744 - acc: 0.5125 - val_loss: 2.3351 - val_acc: 0.1500\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 0s 208us/step - loss: 2.0742 - acc: 0.5125 - val_loss: 2.3350 - val_acc: 0.1500\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 2.0743 - acc: 0.5125 - val_loss: 2.3348 - val_acc: 0.1500\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 219us/step - loss: 2.0731 - acc: 0.5125 - val_loss: 2.3320 - val_acc: 0.1500\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10,input_shape=(28*28,),kernel_regularizer=regularizers.l2(10)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(X,Y,epochs=200,verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>training</th>\n",
       "      <th>testing</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  training  testing  baseline\n",
       "0      1    0.1000      0.1       0.3\n",
       "1      2    0.1125      0.1       0.3\n",
       "2      3    0.1125      0.1       0.3\n",
       "3      4    0.1125      0.1       0.3\n",
       "4      5    0.1250      0.1       0.3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = pd.DataFrame()\n",
    "accuracy['epoch'] = hist.epoch\n",
    "accuracy['epoch'] = accuracy['epoch']+1\n",
    "accuracy['training'] = hist.history['acc']\n",
    "accuracy['testing'] = hist.history['val_acc']\n",
    "accuracy['baseline'] = Y.std()\n",
    "accuracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x191abc83160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd8W/W9+P/XW7Itz8SJY2cnzoLEmWSHWUZCGA0pUDaF0hZ6S0ppL/dCfi3QQgdQ2t7b72WlQIG2lN0SIIWEskohECdAhpOQnTjDdmzHS16SPr8/jmTLsmTJiazhvJ+Phx+Wjo7Oefv46K3P+ZzPEGMMSimlehdbvANQSikVfZrclVKqF9LkrpRSvZAmd6WU6oU0uSulVC+kyV0ppXohTe5KKdULaXJXSqleSJO7Ukr1Qinx2vGAAQNMYWFhvHavlFJJae3atYeNMfnh1otbci8sLKS4uDheu1dKqaQkInsiWU+rZZRSqhfS5K6UUr2QJnellOqF4lbnHkxrayulpaU0NTXFO5SkkZ6ezrBhw0hNTY13KEqpBJJQyb20tJScnBwKCwsRkXiHk/CMMVRWVlJaWsqoUaPiHY5SKoFEVC0jIgtFZKuIbBeRO0Ksc5mIlIjIJhF59miCaWpqIi8vTxN7hESEvLw8vdJRSnUStuQuInbgIWA+UAqsEZHlxpgSv3XGAUuBU4wx1SJScLQBaWLvHj1eSqlgIqmWmQ1sN8bsBBCR54CLgBK/db4DPGSMqQYwxpRHO9Bk4WxxUdvoanuemWanT4bWhyulYiuSapmhwD6/56XeZf5OAE4QkX+LyGoRWRhsQyJyo4gUi0hxRUXF0UXcg44cOcLDDz/c7fedf/75HDlyBID91Y2U1zW1/eytcuLxGO666y7efvvtaIeslFJBRZLcg133B86qnQKMA74CXAk8LiK5nd5kzDJjzExjzMz8/LC9Z2MuVHJ3u91dvm/FihXk5ubS4nLT2OpmcN90pgzLZdSALDzGUN/s4p577uGcc87pqdCVUqqDSJJ7KTDc7/kw4ECQdV41xrQaY3YBW7GSfVK544472LFjB9OmTWPWrFmceeaZXHXVVUyePBmAxYsXM2PGDCZOnMiyZcva3ldYWMjhw4fZuHU7i8+cw+23LmHixIlcvOgCWpuaqG1q5frrr+ell15qW//uu+9m+vTpTJ48mS1btgBQUVHB/PnzmT59OjfddBMjR47k8OHDsT8QSqmkF0md+xpgnIiMAvYDVwBXBazzd6wS+1MiMgCrmmbnsQT2s9c2UXKg9lg20UnRkD7c/dWJIV+/77772LhxI59//jnvvfceF1xwARs3bmxrZvjkk0/Sv39/GhsbmTVrFpdccgl5eXlt769vcrF31w5uefF5pj3xOJdddhn/WvUGCxZ/vdO+BgwYwLp163j44Yd58MEHefzxx/nZz37GWWedxdKlS3nzzTc7fIEopVR3hC25G2NcwBLgLWAz8IIxZpOI3CMii7yrvQVUikgJ8C7wX8aYyp4KOlZmz57dof3473//e6ZOncrcuXPZt28f27Zta3vN5fbgbHEzYmQh06ZNA2DGjBkcPlSKy+3B5fZ02PbFF1/cts7u3bsB+PDDD7niiisAWLhwIf369aO2sZUDRxo5VNOExwTWhimlVHARdWIyxqwAVgQsu8vvsQF+5P2Jiq5K2LGSlZXV9vi9997j7bff5uOPPyYzM5OvfOUrHdqXN7V6MEB6uqNtmd1uR4yV1F2ejonZ4XC0reNyWa1rTJDkXVbbRLa0YDBkOezkpGvLG6VUeDq2jJ+cnBzq6uqCvlZTU0O/fv3IzMxky5YtrF69usPrHo+VxCXg/rNNrLbokRS6Tz31VF544QUAVq5cSXV1NS1uD/2z0gBobvV09XallGqTUMMPxFteXh6nnHIKkyZNIiMjg4EDB7a9tnDhQh599FGmTJnCiSeeyNy5czu8t61kHtC2SERItUlEVSp33303V155Jc8//zxnnHEGgwYPJisrmyyHnSONQrOr61Y7SinlI8GqAmJh5syZJnCyjs2bNzNhwoS4xHOsymubOFTbxKShfbEF9BrdUV6PCIzOz+5yG83NzdjtdlJSUvj444+58abv8pcV7zOuIJv9R5qwhdhGMh83pVT3iMhaY8zMcOtpyT1KXB6DXaRTYgdIsQtNQapUyuuaOlS1VB3cw/XXXEWry43Dkcav//chABwpdhwpNuqbXZ22oZRSwWhyjxK3x2C3Bx/nJdVuo66pY2Judrk5VNNEis2GTawvh5yC4RSvXUfJwVoyUu2kpdhwNruw2QRHqo1qpwe3x4PdprdKlFJd0+QeJS6PwW4LldytOnf/xOwbf2ZsQRZpKXYO1jRyuL6FmsYWjDE4W1y43DbSUqz1HSl2AJpdHjLTNLkrpbqmWSJKXB4PKSFK1Cl2a3mru/3+Rm1jK+mpdtK8SbtPeirGGA7VNLeN9Nji9pCear3u8CZ5bTGjlIqEJvcocXsMKaFK7t6k7+vIZHV4ctHHr816ZpqdFJsNl8dDbkaqX4nd+p2WYkMQml2a3JVS4WlyjxK3u+tqGWgvudc2uTBAn4z2WjERoU+69bxPRmpb4vcld5sIaSk2bQ6plIqIJnc/Rzvkr8cYnv7DwzQ3NbYt8x8GuK1axtvRqbaxlVS7jQxvlYtPXnYafdJTyXGk0D/LepyR1v4F4EixacldKRURTe5+jja5uz2GvzzxCK1+yd03DDCA3SbYRXC5DR6PNQRwn4zUTrMoZaSlUDggC5tNSE+1Uzggq8PVgCPVSu7x6puglEoe2lrGj/+Qv/Pnz6egoIAXXniB5uZmvva1r/Gzn/2MhoYGLrvsMkpLS3G73dx5552U7j9IedkhvnbBuRQU5PPuu+9SWFhIcXEx9fX1nHfeeUyePofP137CsGHDuO+RZxiZl8maNWv41re+RVZWFqeeeir/+Mc/2LhxY8j4HCl2jDG0uD1trWeUUiqYxE3u/7gDDm2I7jYHTYbz7gv5sv+QvytXruSll17i008/xRjDokWL+OCDD6ioqGDIkCG88cYbgDXmjN2RyW9/91veeOttCocN6rTdbdu28euHnuCXv/s/fnjjdbzz5mvMXPIdvvnNb7Js2TJOPvlk7rgj6LzjHfi3mNHkrpTqSuIm9zhbuXIlK1eu5KSTTgKgvr6ebdu2cdppp3Hbbbdx++23c+GFF3LaaadxxNkCELIT06hRo5g6bRo1ja2MLZrC4YP7qa2poa6ujpNPPhmAq666itdff73LmNqSe5B692aXm7v+vonvnD6KsQU5bcuNMdz56kZ2VjQwuG8GD1w6JeSN32CWf3GAw3XN3HDqqLb9/NeL6zlc39xpXZsI3z9rLHNG53V6TSkVW4mb3LsoYceCMYalS5dy0003dXpt7dq1rFixgqVLl7JgwQJu/tHtACGbQjocDvpmptLi8pCeloK4mo+q3jzFbiPFFnwAsY+2V/J88T7SUmzcu3hS2/Ivy+r58+q9DOmbzkc7Krly9nBmFvaPeJ9//PcuNh+s5crZI8hIs/PR9kqWf3GAiUP6kJnW8erhi9Ia/v75fk3uSiUAvaHqx3/I33PPPZcnn3yS+vp6APbv3095eTkHDhwgMzOTa665httuu41169bh8hgys7JxNtSH3Haf9FTGFGSTl+0gLcVGv379yMnJaRs6+LnnnosoRkeKPWjJfWXJIQBWlZR1+OJY5V3+zLfmkGoXVpaURbQfsL7gdlY00NTq4V/bKtr2k5Vm55XvncyL3+34M3VYX3ZUNES8faVUz9Hk7sd/yN9Vq1Zx1VVXMW/ePCZPnsyll15KXV0dGzZsYPbs2UybNo1f/OIX/OQnP8HtMXz96uu54PzzOfPMMyPe3xNPPMGNN97IvHnzMMbQt2/fsO9xpNg69VI1Bt7eXE5OegqHapvYsL+m7bWVJWVMG57L2IJs5o7O65T8u1LZ0EJNYytgfWl4PIa3N5fzlRMLgtb5jx6Qzc6K0F9wSqnYSdxqmTh59tlnOzz/wQ9+0OH5mDFjOPfcczss21vl5Bvf+S6/vPO/25b5ps4bMGBAhxYwt912W9vjiRMnsn79esC6mTtzZthRPHGk2nA5PR2GO2h1e6ioa+burxZx7+slrCopY8qwXA7VNLG+tIb/OvdEABYUDeTOVzexo6K+Q718KDu9pfCCHAfvbCln3d5qKuqamV80MOj6YwqyeL64hRpnK30zdcYopeJJk3sUuNyekPXtXXnjjTf41a9+hcvlYuTIkTz11FNh3+MrMe8sb8DXTL6qoYUUm3DxScN4c+MhVpWU8Z8LTmTVZqsKZoE3GZ/jTe7ffGoNuRlpQbefmWbn/66aTn6Ogx3eUvgNp47ivn9s4T/+so4Um3DmiQVB3zt6gDXW/I7D9Uwf0S/i46CUij5N7sfIYwyNre4O48RE6vLLL+fyyy/v1nuyHHZyM9I6zOyUahd+tOAE+mamMr9oID9/YzN7K52s3HSIwrxMxhZYSXdw3wxuOWssGw/UBt2222N4/8sK3lh/gOtPGcWO8nocKTaunTuSrYfqqGlsZWZhv5Cl8jHe/ewo1+SuVLxpcj9GDc0u3B5Dn4zYVEPYbTZG5GV2WNZY4eB7s8YCsKBoED9/YzOvfFbK6p2VfPOUUR16wv5owYldbv/s37zHqs1lXH/KKHYebmDUgCyyHCn87vJpYWMb3i+DVLvoTVWlEoDeUD1GtU0ubCLkOBLje3JEXibjB+Xw6Ps7aHWbkPXjoSyYOIhPdlZR42xlR0V9W2k8Eil2GyPzsvSmqlIJQJP7MTDGUNvYSrYjBdtR1Ln3lPlFA2lq9ZCXldbt6pH5RQNxeQxvbTrEvionY8LM+xpoTH5WW129Uip+EqO4maSaWt20uj0M7JMe71A6mF80kP/3znbOGl/Qrd6oANOG5ZKf4+Bnr23CY6xk3R2j87NZWVLGmQ++x/fPGsvF04d1WmdvpZP/eukLfn+l1fv3m39cg7PFmpkq1W7jf684icIBmXzjiU/besLabMJPvzqR00/I77CtJc+uY6Nf089AmWkpPHn9LGwCtzz3GfdfMoWRecH/pla3h+ue/JQDR6wB4Gwi/PfCE1k4aTAAn+87wm9WbuXRa2aQFcGV2hFnC9c9+Wlbc1K7Tbj/kikdOpF5PIYlf13H4mlDWTCx89AVoZTVNnHDU2toaHZx5vgC7v7qxA6vv/r5ft7fWsFvA6rTPtpxmMf/tYtHr5nRNmfA0XC2uLjpT2v54fwTjov7Kx/vqGTZBzt47NqZbcft7lc3csKgHK6eMzLO0QWnJfcAu3fvZtKkSeFXpH0YgMCemqG89957XHjhhQAsX76c++7rmV64k4f25UfzT+CmM8Z0+702m3DXhUWcUzSQK2YN54yAZBrOJdOH8bWThtLU6ubJf+8Kus5L60r5ZFcVr31xgNe+OEDJwVomDe3L1OG57Kly8uLafby/tYLiPdWMyc9m6vBcqhpa+PPqPR22s6OintfXHyQ/x8HU4bmdfiYN7UvJwVpeX3+A5V8cYPXOKl5eWxoy9k92VvHRjkpG5GUxdXgutU0unv6ofZ9/Xr2Hf207zHtbKyI6Fis3lfFFaQ3jB/Vh6nCraepfP93XYZ0N+2tYseEQf/z37oi26fPaFwfYdKCWjLQU/vTxnrYvEJ8nPtzFK5/tZ3t5x6uoZz7awztbylm9s7Jb+wv0/tYK/rXtcKf/SW/1zMe7eXdrBR97j1t5XRPPrN7DY+/vTNhRWrXkfgx8k2+khhhTpiuLFi1i0aJF0Q4JsCb+uOXscUf9/q9OHcJXpw45qveOLcjmt5dN45H3dnD/m1s4cKSRIbkZHdZZ5e0lu3JTGQiMH5TD/101HYD6p9awclMZR5yt5Gam8ti1M0ix27jz7xt5aW0pTa3utqkHfdv5nytOYmjAPny2l3/Qth+wOnWFuqm8suQQ6ak2HrtmBhlpdh54cwuPfbCTI84WctJTeWdLuXe/h7hgyuCwx2JlySGG5mbwyDXTERFufe4z3tlSZjWd9Y7x7/sbPt1dxRFnC7mZwZuoBlpVUsaJA3P4+eKJXPLIx7y3tZyLpg0F4GBNI+tLa9rW87WWamp184G3p/GqkrJOV0Hd4Yv7nS3lHf6e3qip1c37X/qO2yHOOCGff24uxxirj8vWsjrGD+oT5yg7673/kWPgcrm47rrrmDJlCpdeeilOp5N77rmHWbNmMWnSJG688UaMMbS6Pfz1yceYPGkSU6ZM4YorrgCgoaGBG264gVmzZnHSSSfx6quvdtrHU089xZIlSwC4/vrrueWWWzj55JMZPXo0L730Utt6v/71r5k1axZTpkzh7rvvjs0BiALfjdy3N3cc7mBflZPNB2vJz3FQvKeK4t1VHW76zi8ayP4jjbyx/iBnjS9oSxrziwbS2Ormw22H29ZdVVLGxCF9QiZ2sNr4+/aTn+Ngy6E69lU5O61njOHtkjJOG5dPhvdKbH7RQNwew7tby1m7p5qqhhbyvR26Wt1dT5ribHHxr22HmV80sK210vyiQVQ7W1m7p7rD35Cf48DtMW1fHuFUN7Swxnvcpg3vx4DstLZkC/C293F+jqNt+AmwqmScLW7ycxy8vTnynsqBXG4P72wtJz/HwRFnK8V+f09v1OG4lZRjjGFVSRkDsh0ArNoU+ZAesZSwJff7P72fLVVborrN8f3Hc/vs28Out3XrVp544glOOeUUbrjhBh5++GGWLFnCXXfdBcC1117L66+/ztSTz+KJh/+HfXt243A42mZe+sUvfsFZZ53Fk08+yZEjR5g9ezbnnHNOl/s8ePAgH374IVu2bGHRokVceumlrFy5km3btnUadvj0008/9oPRw8YWZDM6P4tVJWV8Y15h23Jfsr/rwiK+/9fPAKv5ps/ZEwYisoEWt6fD8rmj88hxpLCqpIxzigZSUdfMur3V/CDMFcqCiYP4/TvbO+xzVUlZ2yiXPpsO1HKgpolb55/QtmzqsFwKchysKiljaK7VzHPpeeP50QtfsGZXFSePHRByv//adphml6fDF9cZJ+aTZrexqqSMOaPz2FPZwNayOn5ywQSWfbCTVSVlQe9RBHpnSzkeAwsmDsRuE86ZMJDX1x+k2eXGkWJnZUkZowZk8bWThvK7t7+kvK6Jgpx0VpWUke1I4UfzT2DpKxvYuL+WycPCD3kRaM3uao44W/nN16ey9JUNrCopY24vHizOd9z+c/4J3PHKBlbvrOLD7Ye5Zs5I1u2tZtXmMr5/DFfKPSVhk3s8DR8+nFNOOQWAa665ht///veMGjWKBx54AKfTSVVVFRMnTqRozpmML5rE1VdfzeLFi1m8eDFgDRe8fPlyHnzwQQCamprYu3dvl/tcvHgxNpuNoqIiysrK2rYTbNjhZEjuYJV8l32wkxn3rmpbVtfsYlxBNhdOGcwvV2wGYNLQ9kva/BwH00f0Y+P+Gk4/oT15pqXY+Mr4Al5aV8rbm8tocXswhrBNPScO6cOQvul4DFw4ZTD/751trCw5xA2njuLxf+3kkfd2ANb9E5vA2ePbe9/abMI5RQN57tO92G3CyWMGsHDSIJa+soFvP1PcaZpEf84WN33SU5g9qv3mabYjhZPH5vH0x7v522f7afHeszl34iB2Hm7guU/3MvPnq7j1nBO4Zu5Iljy7jo93dK4bb2hxMahPOpOH9m07zs+t2cecX/4TuwhVzha+c9po5hcN5LervuTs37xPmt1GTWMr504axLkTB/Hjv23gimUft1Vx3Xj6aG46YwxLX1lvVWN1obHVTVqKjYWTBvH6+gM88/Fu/v7Z/i7fk6hS7TYeuWY64wbmcOkjH1FR13ko65rGVs6d6D1uf9/I9X/8lBbvF3dedhq/fmtrh3M8EnecN56vzxwerT8jqIRN7pGUsHtK4PR3IsL3vvc9iouLGT58OD/96U9pamqi1ePh6edfYffGYpYvX869997Lpk2bMMbw8ssvc+KJHet2fUk7GIfD0fbYd7nc1bDDyeD6kwtpbrXGwfG3cOJgRIRfXTwZ6Hy8f3zBBEqrG8lM63h6fv+ssfTLTG3rnTu4bwZFg7uu6xQRfnXJFIwxiAjziwby6Ps7qW5o4amPdpObmcq8MVapc/ygPuRlOzq8/6bTR5NqEzwGvj5zGJlpKfzq4sms2xu+KmLu6DxSA+qib1twIsP7ZWKw/obCvCyG989s2897X1bw59V7OHtCAa+vP8i80XmMKejcuucrJxS0HbfTT8jn5jPHtN1UTbHZuO7kQob0Tee/F57YofXP1XNG0j8rjZ8vnkzJQatefvXOKp75eA9XzBrBC8WlTBuey4TBXY89NG14P7IcKfznghMZ5vf3JJuX1+7n5XWlzBmVx5ZDdSyaOqTDxPVgHber5oygX1Ya9140iZKDNeRlOZg9qj8nDMymsr6FFnf3Jq4P1WIrqowxYX+AhcBWYDtwR5DXrwcqgM+9P98Ot80ZM2aYQCUlJZ2WxdquXbsMYD766CNjjDHf/va3zYMPPmgKCgqM0+k0dXV1ZuLEieauu+4yX+ytMp98sdkYY0xLS4spKCgw1dXVZunSpebmm282Ho/HGGPMunXrjDHGvPvuu+aCCy4wxhjzxz/+0dx8883GGGOuu+468+KLL7bFkJWVZYwx5q233jKzZ882dXV1xhhjSktLTVlZWaeYE+G4JYvP9labkbe/bn75RokZefvr5q+f7Il3SB384YMdZuTtr5tfeOPbVlbX4/t8/tO9HY7J2j1VPb7PRHHTM8Vmzi/eNkueXWem37PSuNyeeIcUFlBsIsjbYUvuImIHHgLmA6XAGhFZbowpCVj1eWPMkqh848TZhAkTePrpp7npppsYN24c//Ef/0F1dTWTJ0+msLCQWbNmYQy0ulx8/6YbcNbXYYzhhz/8Ibm5udx5553ceuutTJlilRgLCwvDzrIUzIIFC9i8eTPz5s0DIDs7mz//+c8UFAQfuEuFN2VoXwpyHDz+4S5ErDr+ROIbG+iJD3cxekBWW0uXnnTWhAJE4PEPd5Gf42DasNwe32eimF80kDc3HWLFhoNcfNLQbvcLSWRiwtwxF5F5wE+NMed6ny8FMMb8ym+d64GZ3UnuM2fONMXFxR2Wbd68mQkTJkQcfDw1tbr5sqyOEf0zI26+1lOS6bglgv/vbxt49pO9TB+RyyvfOyXe4XSy4Hfv82VZPTedPpql58fm/3rpIx9RvKeaK2cP51cXT4nJPhNBdUMLM3/xNm6PYdm1M7rVkSxeRGStMSbs+OCRNIUcCvj3vCj1Lgt0iYisF5GXRKRn7xQkAF9TuMA6VZX4fEMgzy9KzA+yr5VQd8cFOqZ9TvQdk8S6kulp/bLSmFXYj/RUG6eNO/p2/4kokhuqwa5TAov7rwF/NcY0i8h3gaeBszptSORG4EaAESNGdDPUxHIsHZhUfJ02Lp97F09i8bSj66jV07516iiG5GYwY2TsuvVfNWckGal2zjjh+Kvyu/urEzlwpLGtf0NvEUmxsxTwL4kPAw74r2CMqTTG+NoQ/QGYEWxDxphlxpiZxpiZ+fnBvyXDVRMlCpe35O6bDSlekuV4JRK7Tbh27khyjmIM/ljol5XGVXNGdGpF1JOyHSlcO6+wV9U5R2rC4D4Jd+8lGiLJTGuAcSIySkTSgCuA5f4riIh/X+xFwOajCSY9PZ3KysqkSFit3tmX4jkapDGGyspK0tMTa+AypVT8ha2WMca4RGQJ8BZgB540xmwSkXuwmuQsB24RkUWAC6jCahrZbcOGDaO0tJSKisgGZoqnyvpmXB6D1MQ3saanpzNsWPhejUqp40vY1jI9JVhrmWSy6P8+pF9mGk/fMDveoSiljiPRbC2jAhhj2HW4gZEB090ppVSi0OR+FCrqm6lrcnV7liKllIoVTe5HYUe5NQG0JnelVKLS5H4Udh62ZrcZ3c0p6JRSKlY0uR+FHeUNZKbZGZRgc6cqpZSPJvejsKOinlEDsuLaxl0ppbqiyf0o7Dxcr/XtSqmEpsm9m5pa3ZRWN2pyV0olNE3u3bS7sgFj9GaqUiqxaXLvpl0VVjPIUQM0uSulEpcm92463NACQEEfR5g1lVIqfjS5d1O1N7n3i/PsS0op1RVN7t1U1dBCTnqKzsCklEpomqG6qdrZQv8sLbUrpRKbJvduqna2apWMUirhaXLvpuqGFvplJub0bEop5aPJvZuqGlrop9UySqkEp8m9m6qdLfTXahmlVILT5N4NTa1unC1uLbkrpRKeJvduOOJsBdDWMkqphKfJvRuq2jow6Q1VpVRi0+TeDdVO7Z2qlEoOmty7wVdy12oZpVSi0+TeDW0ld03uSqkEp8m9G6obrBuquRla566USmya3Luh2tlCn/QUUnTQMKVUgtMs1Q1VDTpomFIqOWhy74Zqpw49oJRKDprcu6GqQYceUEolB03u3aCDhimlkoUm9wh5PIbyumYG6typSqkkEFFyF5GFIrJVRLaLyB1drHepiBgRmRm9EBPD4YZm3B7DoD7p8Q5FKaXCCpvcRcQOPAScBxQBV4pIUZD1coBbgE+iHWQiKK9tBqBAk7tSKglEUnKfDWw3xuw0xrQAzwEXBVnvXuABoCmK8SWMQzXWn6Uld6VUMogkuQ8F9vk9L/UuayMiJwHDjTGvd7UhEblRRIpFpLiioqLbwcZTWZ2V3AdqcldKJYFIkrsEWWbaXhSxAb8D/jPchowxy4wxM40xM/Pz8yOPMgGU1TRhExiQra1llFKJL5LkXgoM93s+DDjg9zwHmAS8JyK7gbnA8t52U7WstpkB2Q4dekAplRQiyVRrgHEiMkpE0oArgOW+F40xNcaYAcaYQmNMIbAaWGSMKe6RiOPkUG2TVskopZJG2ORujHEBS4C3gM3AC8aYTSJyj4gs6ukAE0WZJnelVBJJiWQlY8wKYEXAsrtCrPuVYw8r8ZTXNTNjZL94h6GUUhHRCuQINLvcVDW0aDNIpVTS0OQeAV8HJq2WUUolC03uESirtdq4F+i4MkqpJKHJPQJl3pL7oL5acldKJQdN7hHYsL+GFJswrF9mvENRSqmIaHKPwKqSQ8wZ3Z9qc2QlAAAaOUlEQVRsR0SNi5RSKu40uYexo6KeHRUNzJ8wMN6hKKVUxDS5h7GqpAyA+RMHxTkSpZSKnCb3MFaVlDFxSB+G5mbEOxSllIqYJvcutLo9rC89wqnjBsQ7FKWU6hZN7l3YV+Wk1W0YV5AT71CUUqpbNLl3YUdFAwBj8rPiHIlSSnWPJvcu7KyoB2B0fnacI1FKqe7R5N6FHRX1DMh20DcjNd6hKKVUt2hy78KOigatklFKJSVN7l3YWVGvVTJKqaSkyT2EqoYWqp2tWnJXSiUlTe4h+G6mjtGSu1IqCWlyD2GHJnelVBLT5B5CaXUjNoEhuTqGu1Iq+WhyD6Gston8HAcpdj1ESqnko5krhEO1zTpnqlIqaWlyD6G8tkmTu1IqaWlyD+FQbRMDdUJspVSS0uQeRFOrmyPOVgZpyV0plaQ0uQdRXtsMQIEmd6VUktLkHkRZXROA1rkrpZKWJvcgymqt5K7VMkqpZKXJPYhDNb6Su95QVUolJ03uQZTXNeNIsek47kqppBVRcheRhSKyVUS2i8gdQV7/rohsEJHPReRDESmKfqixc6jGauMuIvEORSmljkrY5C4iduAh4DygCLgySPJ+1hgz2RgzDXgA+G3UI42hstomrW9XSiW1SErus4HtxpidxpgW4DngIv8VjDG1fk+zABO9EGOvrLaJAq1vV0olsZQI1hkK7PN7XgrMCVxJRG4GfgSkAWdFJbog7v/0frZUbempzQNwOKeKEpPON9/MPLYN1R2EI/s6LhOBvLGQ0e/Ytq2USlrj+4/n9tm39+g+Iim5B6t47lQyN8Y8ZIwZA9wO/CTohkRuFJFiESmuqKjoXqQxYgy4PYYUexTq2xuPAAYy+7f/uJqhuTbsW5VS6lhEUnIvBYb7PR8GHOhi/eeAR4K9YIxZBiwDmDlz5lFV3fT0t111Qwsnvb+KK6cW8c1TRh3bxp5cCKkD4OrX25fdPwqGnwQLf3Ns21ZKqS5EUnJfA4wTkVEikgZcASz3X0FExvk9vQDYFr0QY6u+2QVAliOS770wnJWQmddxWWaetVwppXpQ2AxmjHGJyBLgLcAOPGmM2SQi9wDFxpjlwBIROQdoBaqB63oy6J7kS+7ZmtyVUkksogxmjFkBrAhYdpff4x9EOa64aYhWcve4obE6eHI/sufYtq2UUmFoD9UAUauWaaoB4wmS3PtryV0p1eM0uQdoaHYDUSi5+xJ4Zv+Oy33J3SR1VwClVILT5B6gvrkVgCyH/dg25KyyfndK7nngboGWhmPbvlJKdUGTe4D6qJfcg9S5+7+ulFI9QJN7gIZo1blrcldKxZEm9wANzS4cKTZS7cd4aMIm96pj275SSnVBk3uA+mZX9Nq42x2QGjA+TUb/9teVUqqHaHIPUN/sik7v1MYqq5QeOCa87wZro5bclVI9R5N7gIZoJXdnVecqGYD0XBCbltyVUj1Kk3uA+mYXOVEbeqB/5+U2m1U1o8ldKdWDNLkHaGh2H3sbdwg+royPji+jlOphmtwDRK9aJlxy1zp3pVTP0eQeICqtZTxua6KOYNUyoOPLKKV6XBSKqL1LxK1lWhutafSCaarBmoEpVMm9P+z7BKp3Q+7Izi1qlFLqGGly9+PxGJwt7siS+7OXw673u14ne2CI5YOgoQL+dyqc+0uYd3P3g1VKqS5ocvfT0GINPRBRa5mqnTDyFJj+jeCvpzjgxPOCvzbvezBgHLzxn9Z2lFIqyjS5+/EN9xtRyd1ZCUUXwdQrur+jjH4w5TJ4/wGte1dK9Qi9oeqnfaKOME0hW5zQ6gxdpx4pbRKplOohmtz9RDx/qm/ogKgkd20SqZSKPk3ufiIe7jfULEvdldlPS+5KqR6hyd1PxCV3Z5RL7jrlnlIqyjS5+2mIOLmHGKu9uzLzwN2sU+4ppaJOk7ufyKtlolhyB62aUUpFnSZ3PxHPn+qsBMQavvdYaHJXSvUQTe5+aptaSbEJ6alhDouzEtL7gv0YuwnolHtKqR6iyd1PeW0z+TkOJNxYL40hJuLorgydlUkp1TM0ufspr2tiYJ/08Ct2NZxvd2TqfKpKqZ6hyd3PoZomBvZxhF8xWsldp9xTSvUQTe5+ymqbGBRRyT1K1TI65Z5SqodocvdqbHFT2+SiIFxyNyb0/KhHQ8eXUUr1AE3uXmW1TQDh69xbneBqimJy76+tZZRSURdRcheRhSKyVUS2i8gdQV7/kYiUiMh6EfmniIyMfqg9y5fcw1bLRKsDk48OHqaU6gFhk7uI2IGHgPOAIuBKESkKWO0zYKYxZgrwEvBAtAPtaYfaSu5hbqhGa+gBH51PVSnVAyLphTMb2G6M2QkgIs8BFwElvhWMMe/6rb8auCaaQcZCeW0zAAP7Bim5tzhh6wpwt0LFFmtZVEvulVZdfmtj+35GnwF9hkDlDkhJh75Do7O/SBkDe/5tzTYVrN1/fQXs+Gf7oGf9R8GIubGNUcXeno9g2OzIO/DtXwsVX7Y/953X4Xz5VvArWpsdTlgI6X06v+b/OfXtp3wLHPis43qpGTD+wvB/Q1MNVO2CIdPCx5uAIvkPDQX2+T0vBeZ0sf63gH8Ee0FEbgRuBBgxYkSEIcZGWW0TGan24FPsbXgRXrul/bnYITdK8WfmgacVmutg09/a9zP1KvjaI/Dyt6DvMLj8z9HZX6QOfAZPXQDX/h3GnNn59X89CJ882v7c7oAfH7Q+fKp3qtwBfzwPvv40TFwc2Xv+chk4D7c/n3olfO3R0OsDHN4Oz14W+vX598Ipt3RevvFlWL6k437+/t3OyR3gmpdh7Dldx7H6Ufjwt7B0/7H3Ro+DSCIO1l0z6Bi1InINMBM4I9jrxphlwDKAmTNnJtQ4t4dqrTbuQXun1pdZv5estZKXow9kRbHkDlbp3befgiKoP2Q9rj1ofZnEWt3Bjr87vX4I+hVayf+Lv8L791slnWjdaFaJJ9w5EcjdaiX2ud+D2TfCC9+wzptwfOf+xX+AYbM6vvbIye2fk1DvK5jYvp+6MihaDOf81HpeewCeOt9aHk7dAavxRNMRyBoQfv0EE0lyLwWG+z0fBhwIXElEzgF+DJxhjGmOTnixU17bHLqljLMSHH1hwNjo79h/CALffvoOs05gX7PL1Izo7zcc332AUPcDnJWQPciqjskb175Mk3vvFe6cCNRYbf3uP9o6T/oMsZJrpPspKLLe5y9zQBfnZBWk5UDucGs/vs9Pv5Ht2/El6Uj+Bv+/NwmTeyStZdYA40RklIikAVcAy/1XEJGTgMeARcaY8uiH2fOsknsXyT2zX8/s2H/wMGeVtZ+M/uCshpZ6q8omHmPPtJ3YIfbdWN2eyH3HRm8M927dTe6BM5ZlRNjst6uZzrqavcz3OfXtp9VpzZeQ4bedtGywpUaY3Ks6xpNkwiZ3Y4wLWAK8BWwGXjDGbBKRe0RkkXe1XwPZwIsi8rmILA+xuYRkjLF6pwa7mQrR65EajP/4Mr5hDXw3WX0nVVONdYkbS+FObP9Suo5ueXxoOyci/D8HNhvO7B9ZQcX3voxgyb2LpsO+z6mvBVqwlm0i1vPuxJGk53VEdwmMMSuAFQHL7vJ7HObORGKraWyl2eWhICdEM0hnJWQX9MzO/evcffvJ7A+tDVCzv329xuqeiyGYrpJ7Wy9d34dWx6U/LnS3JBuYXDPzrNJ0ixPSMrveT1o2pAYpbGXmWS1YQu3PVzhyNUJNacf9+2+jO1cQSXpeaw9VoMzbDDIuJff0vtYNU2elX8nDu6/KbX4xxPgE66papqUe3C2a3I834arqQq2fEXCFF67U3NW9my5L7pUdPz+Ht3Xcb9s2Iuhb4ivA+LabhDS5E8HQA9EaBTIY32ViYLUMtJ+cEPtLQ98JHeyDGHi5nZpptcVP0g+BitCx1rlHWgjo6vOWmQfNIaopAwtHh7/suF//bYSLoakGjDuyeBOUJnf8eqfmBEnurU1WFUlGD91QBevkrz3Yvh/fh6Fye/s6cSu5B9lvYIlMJPKbZSp5+Z8TJoKWzM4qSM1qb+0V6fwFzsrg9e3Q/jkMPNdcLdBSZ70v8PMTeBUQScnd//UkPa81uQPl3uReEGzogcaAUmpPyMxrr4IJVvKA2Cf3Rr+bSR5Px9eCja8T6U0qlbx8/19Pq1U1F8n6gecIhE+WXc10Fqpqp+1z2r/j50dsVtVn4DYaqzuf1x22V91520lGkztWyT03M5X01CCdhaI9lkwwmf2henf7fnz7qt5tdZjyjyMW3C5oPGLt27ity2B/wY6JjpHT+zmrunc+BtadR5rcu7rHFapqx/+c9P/8ZPTr3Gs6Mw+Mx+qc1FXsYP29SXpea3LHuqEacjTImCR378nme+y79DQeq+NHalZsLw2bjgAGBvg6JwXsO1g7ZB2XvndztUBzrd85cRTJPT0XkK7f69vPsSR3336MJ/h2IvmS8W1vwLikPa81uWNVy4ScpCNWyd3/sT21/VLSVxKJ5aVh24l9gvd5kEtgsXk/RF6a3Hs33/kX6pwIJvDGqD0FMnK7Pk/8q1eCiSS5+/bjv36HbURQ9+//GUjS81qTO1a1zKBQQ/06w5xs0eB/88i/N5/veayrPPxLLf7P/V/P6GdNE+iT2d+qynG7YhOjiq1w50TQ9wSpXgk3rWRXvVP9l4dM7gGfn2A3ZjMiTO62FGv8pHh0IoyC4z65uz2Girow48pAD7eW8fsA+PbjW5YRx+Se10VyD9a8DNN1PaZKXuHOiUChqlfCXeGFu1JOcVgdnDpVFQb0avXvFRsokiaZgc2S/W+wJonjPrkfrm/GY+iiWqbKqiKxp/ZcEL4TyH8//h2EYl3l4fughCy5BymR6RAEvZvv/9p/lFUlF7bFizcZBhaKwlUxRjLTWbCpKX03e1PSOr6/qzr3cHH4hjLwjyuJHPfJPez0ej3Zgckn2InYKbnHsOTgS+a5I61BlgI/BEGTe4RtmFVy8v1fs/LDV634r9/drv+R3OMKVtgJ1TIn2HbSsqz5B7osuQd0iErC8/q4T+6HasJMrxeT5B5wKRm4rKteeT3BWWn1Ok3LjOyD5IvT95rqffyrPSK5kgyZ3Pt33Qmqq0HD2rYR6pwM8fkJ5N8rvKv4/dvMJ+F5fdwn97I677gyCV1yj/GloX/JPLCkFThomE8SfwhUBJyV7dUekQy81VXJ3dVkDSAWbj+hRJTcuyi5t20jzBVERnIn9+SbO6obVm46xLtbreHlx9QVM6XmnU7r5De0cF9qKwPeeyv4Rqp2wcBJPRkmOHKsO/PBWs1k9m/vDfiP/+7c264n7P7Q78ZufygthuXeac2M2+qhGFiy8j1f94w1b6bqXfb8u+M5seff7edEMF11/Qd47dbgk9D47yeUjP7WbFD++6/aCfnjO+8nZKubftZ5Gupv8PWS9Z3Xn/0p+HR9R2vKZVB4avS2F0SvTe4ej+HOVzdS1+Qi25HCRa6nOMls4gg5HdYbBaSm2bB9WRJ8Q45sa7LdniQCky6FsWe3LxsxD4bPtdrZZvSzZrPZu7pn4/A3br71e+zZ1gBmX/p9+fUdAcMDptFNy4TRZ0L55vahVlXvMuGr1u8xZ1pf+F+GKBD5DJsFWQHDVA+dYd3L2fVB+P2EUngqbF7ecf9pWR0/p8PnWudo/onBtzH2HGvi7lB/Q84QGDnPGnZ4zFlQVtJxCO5jNWJe9LYVgphIBgDqATNnzjTFxcU9tv3P9x1h8UP/5reXTeXi6cPgsdMheyBc/WKP7VMppXqaiKw1xswMt16vrXNfVXIIu004a7y35OCs7vm6c6WUShC9OLmXMauwH7mZ3hszsbgxqpRSCaJXJvc9lQ18WVbP/KJB1oLWRmus9J4cQkAppRJIr0zuq0rKAFhQNNBaEEmvN6WU6kV6ZXJfuamM8YNyGN7fOwlv4MxBSinVy/W65F7V0ELxnqr2UjvEZjYlpZRKIL0uuf9zcxkeQ3t9O8RmTHallEogvaYT074qJz97rYSSAzUM7pvOpKF92l/UOnel1HGm15Tcn1uzl3e2lFHQJ50lZ41FRNpfjMWY7EoplUB6Tcl95aYy5ozK4683zu38orPSmhLO3mv+XKWU6lKvKLnvPtzAtvJ65vvfRPXnrNI27kqp40qvSO6+du2hk7v2TlVKHV+Stp7izr9vZOOBGsAquU8Y3Ke9XXsgZyX0GRLD6JRSKr6SsuS+r8rJn1bvobHFTbYjhUlD+/KDs8eGfkOwaeGUUqoXi6jkLiILgf8F7MDjxpj7Al4/HfgfYApwhTHmpWgH6m+ltxrmsWtnMDIvK/wbgk0Lp5RSvVjYkruI2IGHgPOAIuBKESkKWG0vcD3wbLQDDGZVySFOHJgTWWJvcYKrUYceUEodVyKplpkNbDfG7DTGtADPARf5r2CM2W2MWQ94eiDGDqobWvh0V1Xom6eBdOgBpdRxKJJqmaHAPr/npcCcEOv2uB0rH+XN1EcZsTkTttvDv8FlTYCt1TJKqeNJJMldgiw7qrn5RORG4EaAESNGHM0mSMnOozZnDI7B/YJHFsyIeTDylKPan1JKJaNIknspMNzv+TDgwNHszBizDFgG1hyqR7ONafOvhvlXH81blVLquBFJnfsaYJyIjBKRNOAKYHnPhqWUUupYhE3uxhgXsAR4C9gMvGCM2SQi94jIIgARmSUipcDXgcdEZFNPBq2UUqprEbVzN8asAFYELLvL7/EarOoapZRSCSApe6gqpZTqmiZ3pZTqhTS5K6VUL6TJXSmleiFN7kop1QuJMUfVl+jYdyxSAew5ircOAA5HOZxo0Li6J1HjgsSNTePqnkSNC44ttpHGmPxwK8UtuR8tESk2xsyMdxyBNK7uSdS4IHFj07i6J1HjgtjEptUySinVC2lyV0qpXigZk/uyeAcQgsbVPYkaFyRubBpX9yRqXBCD2JKuzl0ppVR4yVhyV0opFUbSJHcRWSgiW0Vku4jcEcc4hovIuyKyWUQ2icgPvMt/KiL7ReRz78/5cYpvt4hs8MZQ7F3WX0RWicg27+9+MY7pRL/j8rmI1IrIrfE4ZiLypIiUi8hGv2VBj49Yfu8959aLyPQ4xPZrEdni3f/fRCTXu7xQRBr9jt2jMY4r5P9ORJZ6j9lWETk3xnE97xfTbhH53Ls8lscrVI6I7XlmjEn4H8AO7ABGA2nAF0BRnGIZDEz3Ps4BvsSaOPynwG0JcKx2AwMClj0A3OF9fAdwf5z/l4eAkfE4ZsDpwHRgY7jjA5wP/ANrzq+5wCdxiG0BkOJ9fL9fbIX+68UhrqD/O+9n4QvAAYzyfm7tsYor4PXfAHfF4XiFyhExPc+SpeQedpLuWDHGHDTGrPM+rsMa435oPGLphouAp72PnwYWxzGWs4Edxpij6cB2zIwxHwBVAYtDHZ+LgGeMZTWQKyKDYxmbMWalseZUAFhNHIbWDnHMQrkIeM4Y02yM2QVsx/r8xjQuERHgMuCvPbHvrnSRI2J6niVLcg82SXfcE6qIFAInAZ94Fy3xXlY9GeuqDz8GWCkia8WasxZgoDHmIFgnHlAQp9jAmsnL/wOXCMcs1PFJtPPuBqwSns8oEflMRN4XkdPiEE+w/12iHLPTgDJjzDa/ZTE/XgE5IqbnWbIk96hN0h0tIpINvAzcaoypBR4BxgDTgINYl4TxcIoxZjpwHnCziJwepzg6EWuaxkXAi95FiXLMQkmY805Efgy4gL94Fx0ERhhjTgJ+BDwrIn1iGFKo/12iHLMr6ViIiPnxCpIjQq4aZNkxH7NkSe5Rm6Q7GkQkFeuf9hdjzCsAxpgyY4zbGOMB/kAPXYqGY4w54P1dDvzNG0eZ7zLP+7s8HrFhfeGsM8aUeWNMiGNG6OOTEOediFwHXAhcbbyVtN5qj0rv47VYddsnxCqmLv53cT9mIpICXAw871sW6+MVLEcQ4/MsWZJ7wkzS7a3LewLYbIz5rd9y/zqyrwEbA98bg9iyRCTH9xjrZtxGrGN1nXe164BXYx2bV4fSVCIcM69Qx2c58A1va4a5QI3vsjpWRGQhcDuwyBjj9FueLyJ27+PRwDhgZwzjCvW/Ww5cISIOERnljevTWMXldQ6wxRhT6lsQy+MVKkcQ6/MsFnePo/GDdUf5S6xv3B/HMY5TsS6Z1gOfe3/OB/4EbPAuXw4MjkNso7FaKnwBbPIdJyAP+Cewzfu7fxxiywQqgb5+y2J+zLC+XA4CrVglpm+FOj5Yl8sPec+5DcDMOMS2Has+1neuPepd9xLv//gLYB3w1RjHFfJ/B/zYe8y2AufFMi7v8qeA7wasG8vjFSpHxPQ80x6qSinVCyVLtYxSSqlu0OSulFK9kCZ3pZTqhTS5K6VUL6TJXSmleiFN7kodBRH5ioi8Hu84lApFk7tSSvVCmtxVryYi14jIp94xvB8TEbuI1IvIb0RknYj8U0TyvetOE5HV0j52um+87bEi8raIfOF9zxjv5rNF5CWxxlv/i7dnolIJQZO76rVEZAJwOdZgatMAN3A1kIU1xs104H3gbu9bngFuN8ZMweop6Fv+F+AhY8xU4GSsXpFgjfZ3K9ZY3aOBU3r8j1IqQinxDkCpHnQ2MANY4y1UZ2AN1uShfVCpPwOviEhfINcY8753+dPAi96xeoYaY/4GYIxpAvBu71PjHb9ErBl/CoEPe/7PUio8Te6qNxPgaWPM0g4LRe4MWK+rMTi6qmpp9nvsRj9PKoFotYzqzf4JXCoiBdA2h+VIrPP+Uu86VwEfGmNqgGq/SRyuBd431jjcpSKy2LsNh4hkxvSvUOooaElD9VrGmBIR+QnWzFQ2rNEDbwYagIkishaowaqXB2sY1ke9yXsn8E3v8muBx0TkHu82vh7DP0Opo6KjQqrjjojUG2Oy4x2HUj1Jq2WUUqoX0pK7Ukr1QlpyV0qpXkiTu1JK9UKa3JVSqhfS5K6UUr2QJnellOqFNLkrpVQv9P8DO3Eb283jsfIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = accuracy.plot.line(x='epoch',y='training')\n",
    "ax = accuracy.plot.line(x='epoch',y='testing',ax=ax)\n",
    "accuracy.plot.line(x='epoch',y='baseline',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
